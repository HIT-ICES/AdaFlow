# @package _global_

# to execute this experiment run:
# python run.py experiment=dysr.yaml

defaults:
  - override /trainer: default.yaml
  - override /model: coACN.yaml
  - override /datamodule: PWDatamodule.yaml
  - override /callbacks: default.yaml
  - override /logger: tensorboard.yaml

seed: 12351

project_name: "servicecomputinglib"
experiment_name: "coACN"
tags: []

trainer:
  min_epochs: 1
  max_epochs: 1
  gradient_clip_val: 0.5

logger:
  tensorboard:
    name: "coACN"

callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: "val/P@5/dataloader_idx_0" # name of the logged metric which determines when model is improving
    mode: "max" # can be "max" or "min"
    save_top_k: 1 # save k best models (determined by above metric)
    save_last: True # additionaly always save model from last epoch
    verbose: False
    dirpath: "checkpoints/"
    filename: "coACN_{epoch:03d}"
    auto_insert_metric_name: False

  early_stopping:
    _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: "val/P@5/dataloader_idx_0" # name of the logged metric which determines when model is improving
    mode: "max" # can be "max" or "min"
    patience: 100 # how many epochs of not improving until training stops
    min_delta: 0.0001 # minimum change in the monitored metric needed to qualify as an improvement